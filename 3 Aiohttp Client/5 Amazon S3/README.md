# Amazon S3

## Введение

Возьмем для пример следующую задачу - нужно загрузить и сохранить аватарку пользователя. Для этого нужно:

1. Отправить картинку на сервер
2. Как-то ее обработать
3. Сохранить на диск
4. Настроить веб-сервер, который сможет отдать клиенту эту картинку по ссылке

На этом список подзадач заканчивается, но возникают некоторые вопросы. Что делать, если:

-   Нагрузка выросла?
-   Картинок очень много и памяти сервера не хватает?
-   Пользователей очень много, и мы не можем с нужной скоростью отдавать им картинки
-   Пользователи географически распределены по всему миру, а наши сервера находятся в одной локации? Из-за чего возникает большая задержка при передачи данных.
-   Приложение запускается в оркестраторе (например, kubernetes.io) и требует не хранить данные, то есть быть stateless

Решением будет централизованное хранилище, которое поддерживает масштабирование, резервирование и будет географически распределено по всему миру. Позволить себе такое могут только большие корпорации. Это они и сделали, реализовав объектные хранилища, которые решают все проблемы выше:

-   AWS Amazon
-   Digital Ocen
-   MCS Mail.ru
-   SberCloud
-   Selectel
-   Yandex Cloud
-   и т.д.

У каждого из такого облачного провайдера есть свое хранилище, но работают они по общему протоколу - Amazon S3

## Основные понятия протокола

-   Amazon S3 - это хранилище, которое хранит данные в виде объектов в bucket

-   Объект (object) - это файл и любые метаданные, описывающие файл

-   Bucket - это контейнер для данных

Чтобы хранить данные в совместимых с Amazon S3 хранилищах сначала нужно создать bucket и указать имя для bucket и его регион. Затем загрузить данные в bucket как объекты в Amazon S3. Где у каждого объекта есть ключ, который является уникальным идентификатором объекта в корзине.

### Objects

Объекты - это основные сущности, храняющиеся в совместимых с Amazon S3 хранилищах. Объекты состоят из данных и метаданных.

Метаданные - это набор пар имя-значение, которые описывают объект. Эти пары включают некоторые метаданные по умолчанию, такие как дата последнего изменения, и стандратные заголовки HTTP, например, Content-Type. Вы также можете указать собственные метаданные во время сохранения объекта.

### Versions

S3 Предоставляет функции, которые вы можете настроить для поддержки вашего кокретного варианта использования. Например, вы можете использовать управление версиями S3, чтобы хранить несколько версий объекта, что позволяет восстанавливать объекты, которые были случайно удалены или перезаписаны.

### Storage Classes

Большинство S3 совместимых хранилищ предлагают классы хранения (Storage Classes). Эти классы предназначены для различных сценариев использования.

Вы можете хранить "горячие" данные с частым доступом - например, аватарки - в классе, поддерживающим быстрый доступ к данным, но при этом стоит дороже.

А "холодные" данные - например, резервные копии - хранить в классе, который стоит дешевле, но обладает более низкой скоростью доступа.

### Regions

Вы можете выбрать географический регион, в котором Amazon S3 будет хранить созданные вами bucket.

Не все хранилища поддерживают такую возможность. Вы можете выбрать регион для оптимизации задержки, минимизации затрат или соблюдение нормативных требований.

Обратите внимание, что объекты, хранящиеся в регионе AWS, никогда не покидают этот регион, если вы явно не перенесете или не реплицируете их в другой регион.

### ACL

По умолчанию bucket S3 и объекты в них являются частными. У вас есть доступ только к тем ресурсам S3, которые создаете вы. Чтобы предоставить разрешения на объекты, используйте access control list (ACL).

С помощью ACL можно управлять доступом к конкретным файлам, например, сделать его публичным по ссылке для всех.

## Aiobotocore

Смотри `1_aiobotocore.py`

Данная библиотека aiobotocore в точности повторяет описание API от Amazon, например:

-   Метод `client.get_object_acl` = `API_GetObjectAcl`
-   Метод `client.put_object` = `API_PutObject`
-   Метод `client.get_paginator('list_objects')` = `API_ListObjects`

## MultipartUpload

В прошлом разделе мы рассмотрели, как скачивать и загружать файлы без их полного сохранения в оперативной памяти.

Такая загрузка происходит в рамках одного соединения. Если в какой-то момент загрузка прервется то нужно будет передавать весь файл заново.

Этот недостаток нестрашен, если файлы сравнительно маленького размера - 100-1000 Мб. Но когда нужно отправить файл в десятки или сотни гигабайт, могут возникнуть проблемы. Такими файлами могут быть дампы базы данных или логи сервиса.

Такие файлы лучше загружать частями, а потом соединять на стороне хранилища. Для загрузки данных частями в протоколе S3 есть 4 специальных хранилища:

-   CreateMultipartUpload: инициализирует загрузку частями
-   UploadPart: загружает часть
-   CompleteMultipartUpload: завершает загрузку
-   AbortMultipartUpload: отменяет загрузку

### Алгоритм загрузки

1. Вызываем CreateMultipartUpload и передаем в аргументы Bucket и Key. Из ответа нужно получить Uploaddid, идентификатор загрузки. Он понадобится для однозначного определения, к какому файлу относится часть.

2. В цикле вызываем UploadPart. В аргументы нужно передать Bucket, Key, PartNumber, Uploadid, Body, PartNumber, номер части в последовательности (1, 2, 3, ...), Uploadid - id из предыдущего метода. ИЗ ответа нужно получить ETag - по сути, это версия. Если вызвать 2 раза метод UploadPart, то ETag позволит определить, какие данные нужно взять. Так нужно продолжать, пока все части не будут загружены.

3. После загрузки всех частей нужна процедура завершения загрузки (объединение всех частей в один файл). Для этого вызываем CompleteMultipartUpload и передаем в аргументы Uploadid и массив из двух ключей `[{"ETag": ..., "PartNumber": 1}, {...}]`. Функция может вызвать исключения:

    - EntityTooSmall - часть меньше 5 Мб
    - InvalidPart - какая-то модель не найдена
    - InvalidPartOrder - порядок PartNumber не является возрастающем
    - NoSuchUpload - Uploadid не найден

В случае если произошла ошибка при загрузке, можно отменить загрузку методом AbortMultipartUpload. Для этого вызываем этот метод с аргументами Bucket, Key и Uploadid.

### Контекстные менеджер для алгоритма

Смотри `2_multipart_cm.py`
